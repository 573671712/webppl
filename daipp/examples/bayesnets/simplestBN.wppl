/*
This is a simple "explaining away" bayes net. It allows us to test whether the context network learns
to pass along the relevant (summary of) previous choices.

run with: ./webppl examples/bayesnet.wppl --require ./daipp
*/

var targetModel = function(){
  var aStd = 1//sampleDaipp(gammaERP, [2,2])
  var cStd = 0.5//sampleDaipp(gammaERP, [2,2])

  var a1 = sample(gaussianERP, [0,aStd])
  var a2 = sample(gaussianERP, [0,aStd])

  var c1 = sample(gaussianERP, [a1+a2,cStd])
  // var c2 = sample(gaussianERP, [b2+b3,cStd])
  // var c3 = sample(gaussianERP, [b1+b3,cStd])

  return {c1: c1}
}
var data = repeat(20,targetModel)
display(data)


//NOTE: assume that sampleDaipp(erp,params,opt) takes a final options arg, and
//when opt.observedVal is not undefined it will be interpretted as a factor
//(except in fantasy mode....).

var model = function(){
  // initContext("modelLearningInitContext")

  var aStd = 1
  var cStd = 0.5

  //map over observations
  var latents = mapData(data,function(datum){
    initContext(datum) //make depend on context in global model?
    var a1 = sampleDaipp(gaussianERP, [0,aStd])
    var a2 = sampleDaipp(gaussianERP, [0,aStd])

    var c1 = sampleDaipp(gaussianERP, [a1+a2,cStd], {observedVal: datum.c1})

    return {a1: a1, a2: a2}
  })

  return latents
}



// Tutorial training.
// var erp = SMC(model, {particles: 100, saveTraces: true, ignoreGuide: true});
// display(erp.hist)
// var params = Optimize(model, {steps: 1000, method: {adagrad: {stepSize: 0.1}}, estimator: {EUBO: {traces: erp.traces}}});

// VI.
// var params = Optimize(model, {
//   steps: 100,
//   method: {adagrad: {stepSize: 0.01}},
//   estimator: {ELBO: {samples: 1}},
//   verbose: true});

// SampleGuide(model, {samples: 1000, params: params});
//params

var improveParams = function(params) {
  // datumIndex controls which data point will be mapped over by
  // mapData during evaluation.

  // TODO: Compute importance weights for the mini-batch we're about
  // to use for optimization?

  var ess1 = EvaluateGuide(model, {datumIndices: [], params: params, samples: 100});
  // var weights2 = evaluateGuide(model, {datumIndex: 1, params: params});

  display(ess1)
  display('--------------------');

  // Do a few optimization steps.

  return Optimize(model, {
    params: params,
    steps: 100,
    method: {adagrad: {stepSize: 0.01}}, //FIXME: what about state in adagrad, etc?
    estimator: {ELBO: {samples: 1, batchSize: 5}},
    verbose: true
  });
};

var iterate = function(fn, initialVal, n) {
  var iter = function(i, val) {
    return  (i < n) ? iter(i + 1, fn(val)) : val;
  };
  return iter(0, initialVal);
};

iterate(improveParams, {}, 10);

'done';
