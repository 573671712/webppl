// LDA
//
// Run this with:
// LOG_LEVEL=1 ./webppl examples/ldaVariational.wppl

// Known issues:

// It's not uncommon to see the following error:
// "AssertionError: Expected marginal to be normalized."

// I'm not sure what causes this, but one guess is that it's possible
// to sample values on the boundary of the simplex from the logistic
// normal. Such values are outside of the support of the Dirichlet, so
// scoring them may lead to the gradient blowing up.

// This seem to happen more often with larger step sizes.

// What appears to happen is that on some runs, one of the variance
// params of a logistic normal gets driven really high. (e.g. We end
// up sampling from a normal with sd=600 or something.) This can lead
// to sampling extreme values for q, which in turn have a large
// gradient under the Dirichlet score. The question is why do we end
// up with such large variance params? Could it be that we happen to
// make a large update to the parameter simply as a result of the
// variance of the gradient estimate, but that this is exagerated by
// using exp to map from R to R+? (Check the math.) I've tried using
// soft-plus in place of exp, and it does appear to be better behaved.

var ones = function(dims) { zeros(dims).add(1); };

// TODO: Add these helpers to header.wppl if useful.
var forEach2 = function(xs, f) {
  map(function(i) {
    return f(xs[i], i);
  }, _.range(xs.length));
  return;
};

var times = function(n, f) {
  forEach2(_.range(n), f);
};

var paramInit = function(val, options) {
  return paramChoice(deltaERP, [val], options);
};


var softplus = function(z) {
  return ad.tensor.log(ad.tensor.add(ad.tensor.exp(z), 1));
};

// Hack to call erp logistic without member expression. Useful for
// approximating dirichlet with delta...
var erp = { logistic: logistic };

// This is a ridiculously inefficient way to do this.
var oneHotIndex = function(x) {
  var d = ad.value(x).length;
  return ad.tensor.sumreduce(ad.tensor.mul(Vector(_.range(d)), x));
};

var lda = function(corpus, vocabSize, numTopics, alpha, eta) {

  var topics = repeat(numTopics, function() {
    var q = sampleGuide(logisticNormalERP, [
      paramInit(zeros([vocabSize - 1, 1]), { prefix: 'topic_mu' }),
      softplus(paramInit(zeros([vocabSize - 1, 1]), { prefix: 'topic_log_sigma' }))
    ]);

    return sample(dirichletERP, [eta], { guideVal: q }); // beta_k
  });


  forEach2(corpus, function(doc) {
    // gamma_d
    var q = sampleGuide(logisticNormalERP, [
      paramInit(zeros([numTopics - 1, 1]), { prefix: 'props_mu' }),
      softplus(paramInit(zeros([numTopics - 1, 1]), { prefix: 'props_log_sigma' }))
    ]);


    var topicDist = sample(dirichletERP, [alpha], { guideVal: q }); // theta_d


    // Note, that this forEach should not use mini-batches.
    forEach2(doc, function(count, word) {

      // TODO: More efficient summing out of z.
      // Move the factor inside Enumerate. (Need a version of
      // Enumerate which doesn't normalize? Check math.)

      if (count > 0) {
        var marginal = Enumerate(function() {
          var z = sample(discreteERP, [topicDist]);
          var topic = topics[z];
          return sample(discreteERP, [topic]);
        });

        factor(count * marginal.score([], word));
      }

    });

  });

  return topics;

};


// Each document is represented by an array of word counts. Therefore
// doc.length == vocabSize, and sum(doc) = no. of words in doc.

var bars = readJSON('bars2.json');

var vocabSize = 4; // V
var numTopics = 4; // K

// Parameter for prior on topic proportions.
var alpha = Vector(repeat(numTopics, constF(0.1)));
// Parameter for prior on topics.
var eta = Vector(repeat(vocabSize, constF(0.1)));



var marginal = Variational(function() {
  return lda(bars, vocabSize, numTopics, alpha, eta);
}, {
  optimizer: { adagrad: { stepSize: 0.5 } },
  //optimizer: { adam: { stepSize: 0.01 } },
  steps: 500,
  samplesPerStep: 1,
  returnSamples: 1,
  callback: function(i, parameters) {
    var mus = [parameters.topic_mu0,
               parameters.topic_mu1,
               parameters.topic_mu2,
               parameters.topic_mu3];

    // var sigmas = [parameters.topic_log_sigma0,
    //               parameters.topic_log_sigma1,
    //               parameters.topic_log_sigma2,
    //               parameters.topic_log_sigma3];

    // The topics used to generate the bars2 data are 2x2 horizontal
    // and vertical stripes.

    // [.5, .5, .0, .0]
    // [.0, .0, .5, .5]
    // [.5, .0, .5, .0]
    // [.0, .5, .0, .5]

    display(_.map(mus, logistic));
    //display(_.map(sigmas, ad.tensor.exp));
  }
});


// The following will write the history of the elbo estimates to disk
// using a filename based on the parameters used for inference. The
// filename will also include a timestamp so multiple runs can be made
// without worrying about overwrite previous data.

// All of the files in a single directory can be viewed on a single
// plot using:

// python viz.py dirname

// (I have Python 2 installed.)

// Extra information about a particular set-up should be added to the
// *front* of the filename as the Python script expects the timestamp
// to come at the end. (Just before the extension.)

writeJSON('histlda/' + marginal.info + '.json', marginal.history);

'done';
