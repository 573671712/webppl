var X = readJSONDataSet('mnist_inputs.json');

var zDim = 2;
var hDecodeDim = 200;
var hEncodeDim = 200;
var xDim = 784;

var observe = function(erp, params, val) {
  factor(erp.score(params, val));
};

var forEach = function(arr, fn) {
  if (arr.length > 0) {
    fn(first(arr));
    forEach(rest(arr), fn);
  }
};

var curBatch = function(arr, batchSize) {
  assert.ok(arr.length % batchSize === 0);
  var numBatches = arr.length / batchSize;
  var i = getCurStep() % numBatches;
  return arr.slice(i * batchSize, (i + 1) * batchSize);
};

var encode = function(x, W, b, returnMean) {
  var W0 = W[0], W1 = W[1], W2 = W[2];
  var b0 = b[0], b1 = b[1], b2 = b[2];
  var h = ad.tensor.tanh(ad.tensor.add(ad.tensor.dot(W0, x), b0));
  var mu = ad.tensor.add(ad.tensor.dot(W1, h), b1);
  var sigma2 = ad.tensor.exp(ad.tensor.add(ad.tensor.dot(W2, h), b2));
  if (returnMean) {
    return mu;
  } else {
    return sampleGuide(diagCovGaussianERP, [mu, sigma2], { reparam: true });
  }
};

var decode = function(z, W, b) {
  var W3 = W[0], W4 = W[1];
  var b3 = b[0], b4 = b[1];
  var h = ad.tensor.tanh(ad.tensor.add(ad.tensor.dot(W3, z), b3));
  return ad.tensor.sigmoid(ad.tensor.add(ad.tensor.dot(W4, h), b4));
};

var model = function() {
  // Variational parameters.
  var W0 = paramChoice(tensorInitERP, [[hEncodeDim, xDim], 0.01], { name: 'W0' });
  var W1 = paramChoice(tensorInitERP, [[zDim, hEncodeDim], 0.01], { name: 'W1' });
  var W2 = paramChoice(tensorInitERP, [[zDim, hEncodeDim], 0.01], { name: 'W2' });

  var b0 = paramChoice(tensorInitERP, [[hEncodeDim, 1], 0.01], { name: 'b0' });
  var b1 = paramChoice(tensorInitERP, [[zDim, 1], 0.01], { name: 'b1' });
  var b2 = paramChoice(tensorInitERP, [[zDim, 1], 0.01], { name: 'b2' });

  // Model parameters.
  var W3 = paramChoice(tensorInitERP, [[hDecodeDim, zDim], 0.01], { name: 'W3' });
  var W4 = paramChoice(tensorInitERP, [[xDim, hDecodeDim], 0.01], { name: 'W4' });

  var b3 = paramChoice(tensorInitERP, [[hDecodeDim, 1], 0.01], { name: 'b3' });
  var b4 = paramChoice(tensorInitERP, [[xDim, 1], 0.01], { name: 'b4' });

  forEach(curBatch(X, 10), function(x) {
    var guide = encode(x, [W0, W1, W2], [b0, b1, b2]);
    var z = sample(diagCovGaussianERP, [Vector([0, 0]), Vector([1, 1])], { guideVal: guide });
    var p = decode(z, [W3, W4], [b3, b4]);
    observe(mvBernoulliERP, [p], x);
  });

  return 0;
};

var dumpSamples = function(i, params) {
  var fn = 'vae/samples/' + i + '.json';
  writeJSON(fn, repeat(20, function() {
    var z = sample(diagCovGaussianERP, [Vector([0, 0]), Vector([1, 1])]);
    var p = decode(z, [params.W3, params.W4], [params.b3, params.b4]);
    return p.toFlatArray();
  }));
  exec('python vae/viz_samples.py ' + fn + ' vae/samples/' + i + '.png');
};

var dumpLatents = function(i, params) {
  var fn = 'vae/latents/' + i + '.json';
  writeJSON(fn, map(function(x) {
    var W = [params.W0, params.W1, params.W2];
    var b = [params.b0, params.b1, params.b2];
    return encode(x, W, b, true).toFlatArray();
  }, X.slice(0, 1000)));
  exec('python vae/viz_latents.py ' + fn + ' vae/latents/' + i + '.png');
};

var erp = Variational(model, {
  optimizer: 'adagrad',
  steps: 1000,
  stepSize: 0.05,
  samplesPerStep: 1,
  returnSamples: 1,
  callback: function(i, params) {
    dumpSamples(i, params);
    dumpLatents(i, params);
  }
});


// TODO:

// 1. Can we make it possible to sample form the prior without
// significant re-writing of the model?

// 2. Can we make it possible to run this model with other inference
// algorithms without having to change too much. (It would be nice to
// be able to do this in principle, even though it may not be
// practical.)

'done';
